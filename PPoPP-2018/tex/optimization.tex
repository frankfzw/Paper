\section{Shuffle Optimization}
\label{opt}
This section presents the detailed methodologies to achieve three design goals. 
The out-of-framework shuffle data management is used to decouple shuffle from execution and provide a cross-framework optimization. 
Two heuristic algorithms (Algorithm \ref{hminheap}, \ref{mhminheap}) and \hlyellow{a co-scheduler} is used to achieve shuffle data pre-fetching without launching tasks.

\subsection{Decouple Shuffle from Execution}
To achieve the decouple from map tasks and reduce tasks, the original shuffle write and read implementation in the current frameworks should be modified to apply the API of SCache.

\hlyellow{
During shuffle write of map tasks, the partitioned shuffle data blocks will be written on the disks.
Each block contains a part of input data of a particular reduce task. 
To avoid the release of a slot being blocked by disk operation, SCache provides a disk-write-like API named $putBlock$ to handle the shuffle write.
Inside the $putBlock$, SCache uses memory copy to move the shuffle data blocks out of map tasks and store them in the reserved memory.
At the same time, the slot will be released immediately.
From the perspective of reduce task, SCache provides an API named $getBlock$ to replace the original implementation of shuffle read. 
With the precondition of shuffle data pre-fetching, 
the $getBlock$ leverages the memory copy to get the shuffle data requested by a reduce task from SCache.
Instead, the original implementation of shuffle read will trigger a network transfer that keeps CPU idle.
}
\ifrevision
\reversemarginpar
\marginnote{60C}
\fi
%To this end, all I/O operations are managed outside of the DAG framework, and the slot is occupied only by the CPU intensive phases of task.
\subsection{Pre-schedule with Application Context}
\hlyellow{
The pre-scheduling and pre-fetching are the most critic steps toward the optimization. 
The task -- node mapping is not determined until tasks are scheduled by the scheduler of DAG framework. 
Once the tasks are scheduled, the slots will be occupied to launch them. 
On the other hand, the shuffle data cannot be pre-fetched without the awareness of task -- node mapping.
}
To get rid of this dilemma, we propose a co-scheduling scheme with two heuristic algorithms (Algorithm \ref{hminheap}, \ref{mhminheap}). 
\hlyellow{
That is, the task -- node mapping is established ahead of the task scheduling, and then it is enforced to the DAG scheduler when the task scheduling starts.
}

\begin{figure}
	\centering
	\includegraphics[width=0.5\linewidth]{fig/reduce_cdf}
	\caption{Shuffle Time Fraction CDF of OpenCloud Trace}
	\label{fig:cdf}
	\vspace{-1em}
\end{figure}

\begin{figure}
	\centering
	\includegraphics[width=0.75\linewidth]{fig/sim}
	\caption{Stage Completion Time Improvement of OpenCloud Trace}
	\label{fig:sim}
	\vspace{-1em}
\end{figure}

% We explore several pre-scheduling schemes in different scenarios, and evaluate the performance calculating the improvement of reduce tasks completion time with trace of OpenCloud \cite{opencloudtrace}. We first emulate the scheduling algorithm of Spark to schedule the reduce tasks of one job, and take the bottleneck of the task set as the completion time. Then we remove the shuffle read time as the assumption of shuffle data pre-fetch and emulate under different schemes. The result is shown in \ref{fig:sim}.
% \begin{figure*}
% 	\centering
% 	\begin{minipage}{0.34\linewidth}
% 		\begin{figure}[H]
% 			\includegraphics[width=\textwidth]{fig/shuffle_size}
% 			\caption{Shuffle Size Comparing with Input Size}
% 			\label{fig:shuffle_size}
% 		\end{figure}
% 	\end{minipage}
% 	\begin{minipage}{0.65\linewidth}
% 		\begin{figure}[H]
% 			\begin{subfigure}{0.5\textwidth}
% 				\includegraphics[width=\linewidth]{fig/reduce_cdf}
% 				\caption{Shuffle Time Fraction CDF}
% 				\label{fig:cdf}
% 			\end{subfigure}	
% 			\begin{subfigure}{0.5\textwidth}
% 				\includegraphics[width=\linewidth]{fig/sim}
% 				\caption{Stage Completion Time Improvement}
% 				\label{fig:sim}
% 			\end{subfigure}	
% 			\caption{Emulate Result of OpenCloud Trace}
% 		\end{figure}
% 	\end{minipage}
% \end{figure*}
% \begin{figure}
\subsubsection{Problem of Random Mapping}\label{randomassign}
\hlyellow{
The simplest way of pre-scheduling is mapping tasks to different nodes randomly and evenly. 
In order to evaluate the effectiveness of random mapping, we use traces from OpenCloud \footnote{\label{fn:trace}http://ftp.pdl.cmu.edu/pub/datasets/hla/dataset.html} for the simulation.
}
Note that most of the traces from OpenCloud is shuffle-light workload as shown in Figure \ref{fig:cdf}. 
The average shuffle read time is 3.2\% of total reduce completion time.

As shown in Figure \ref{fig:sim}, the baseline (i.e., red dot line) is the stage completion time with Spark FIFO scheduling algorithm. 
And then we remove the shuffle read time of each task, and run the simulation under three different schemes: random mapping, Spark FIFO, and our heuristic MinHeap.
Random mapping works well when there is only one round of tasks. 
But the performance collapses as the round number grows. 
It is because that data skew commonly exists in data-parallel computing \cite{skewtune, reining, gufler2012load}. 
\hlyellow{
Several heavy tasks might be assigned on the same node, thus slowing down the whole stage. 
In addition, randomly assigned tasks also ignore the data locality between shuffle map output and shuffle reduce input, which might introduce extra network traffic in cluster.
}

\subsubsection{Shuffle Output Prediction}\label{shuffleprediction}
\ifrevision
\marginnote{60E}
\fi
\hlyellow{
The problem of random mapping was obviously caused by application context (e.g., shuffle data size) ignorance. 
Note that a balanced schedule decision can be made under the consideration of the number of shuffle dependencies, the number of shuffle data blocks, and the size of each block. 
The number of shuffle dependencies can be easily extracted from DAG information. 
The number of shuffle data blocks equals the number of map tasks $\times$ the number of reduce tasks, which can also be inferred from DAG.
So the pre-scheduling can be made if the "prediction" of size of shuffle block is practical.
}

\begin{figure}
	\centering
	\includegraphics[width=0.9\linewidth]{fig/shuffle}
	\caption{Shuffle Data Prediction}
	\label{fig:shuffle}
	\vspace{-1em}
\end{figure}

% \hlyellow{
% The shuffle size of each reduce task is decided by input data, map task computation, and hash partitioner. 
% Each map task produces a data block for each reduce task. 
% }
The size of each reduce partition can be calculated $reduceSize_i = \sum_{j=0}^{m} {BlockSize_{ji}}$ ($m$ is the number of map tasks). 
$BlockSize_{ji}$ represents the size of block which is produced by map $task_j$ for reduce $task_i$ (e.g., block `1-1' in Figure \ref{fig:shuffle}).

For the most DAG applications with random large scale input, 
the $BlockSize_{ji}$ can be predicted with decent accuracy by liner regression model (i.e., equation \ref{linearregresion}) based on observation that the ratio of map output size and input size are invariant given the same job configuration \cite{ishuffle, predict}.
\begin{equation}
\label{linearregresion}
\begin{aligned}
	BlockSize_{ji} = k \times inputSize_j + b
\end{aligned}
\end{equation}
\hlyellow{
The $k$ and $b$ can be determined using the observed $inputSize_j$ of $j$th map task and $BlockSize_{ji}$.
}

\ifrevision
\reversemarginpar
\marginnote{60D}
\fi

\hlyellow{
Though the linear regression is stable in most scenarios, it can fail in some uncertainties introduced by the sophisticated frameworks like Spark} \cite{apachespark}.  
\hlyellow{
For instance, the customized partitioner might bring large inconsistency between observed map output blocks distribution and the final shuffle data distribution. 
}
We present two particular examples in Figure \ref{fig:hash_pre} and Figure \ref{fig:range_pre_sample}. 
All of the data in both figures are normalized to $0-1$ to fit in one figure. 
% In Figure \ref{fig:range_pre_sample}, we use the distribution of second shuffle of Spark Terasort \cite{spark-tera} that are partitioned by Spark RangePartitioner \cite{apachespark}. 
% In Figure \ref{fig:hash_pre} and Figure \ref{fig:range_pre_sample}, we use different datasets with different partitioners, and normalize the distribution to $0-1$ to fit in one figure. 
The observed map outputs are randomly picked. 
With a random input and a hash partitioner in Figure \ref{fig:hash_pre}, the distribution of observed map output is close to the final reduce input distribution. 
The prediction results also turn out to be good. 
\hlyellow{
However, the data partitioned by Spark RangePartitioner} \cite{apachespark} in Figure \ref{fig:range_pre_sample} \hlyellow{results in a deviation in linear regression model. }
\hlyellow{
It is because the RangePartitioner might introduce an extreme high data locality skew. 
That is, for one reduce task, almost all of the input date are produced by a particular map task (e.g., the observed map tasks only produce data for reduce tasks 0-5 in Figure} \ref{fig:range_pre_sample}).
\hlyellow{
The data locality skew results in a missing of other reduce tasks' data in the observed map outputs.
}

% Several map outputs (marked as Map Output in Figure \ref{fig:shuffle}) are picked as observation objects to train the model and than predict the final reduce distribution.
\ifrevision
\reversemarginpar
\marginnote{60D}
\fi
\hlyellow{
To handle this corner case, we introduce another methodology named weighted reservoir sampling as a substitution of linear regression. 
Note that linear regression will be replaced only when a RangePartitioner or a customized non-hash partitioner occurs. 
For each map task, we use classic reservoir sampling to randomly pick $s \times p$ of samples, where $p$ is the number of reduce tasks and $s$ is a tunable number. 
}
After that, the map function is called locally to process the sampled data (\textit{Sampling} in Figure \ref{fig:shuffle}). 
Finally, the partitioned outputs are collected with the $InputSize$ of each map task which is used as the weight of the samples.
The $BlockSize_{ji}$ can be calculated by
\begin{equation}
\label{equationsample}
\begin{aligned}
	BlockSize_{ji} &= {{InputSize_j \times \frac{sample_i}{s \times p}}} \\
	sample_i &= \text{number of samples for $reduce_i$}
\end{aligned}
\end{equation}
% The classic reservoir sampling is designed for randomly choosing \textit{k} samples from \textit{n} items, where \textit{n} is either a very large or an unknown number \cite{reservoir}. 
In Figure \ref{fig:range_pre_sample}, we set $s$ equals $3$, the result of sampling prediction is much better than linear regression. 
The variance of the normalization between sampling prediction and reduce distribution is because the standard deviation of the prediction results is relatively small compared to the average prediction size, which is $0.0015$ in this example. 
Figure \ref{fig:prediction_relative_error} further proves that the sampling prediction can provide precise result even in the dimension of absolute shuffle partition size. 
On the opposite, the result of linear regression comes out with large relative error. 
Though the weighted reservoir sampling is precise, it also introduced extra overhead. 
We will show the overhead evaluation of sampling in Section \ref{evaluation}.

During the shuffle output prediction, the composition of each reduce partition is calculated as well. We define $prob_i$ as
\begin{equation}
\label{equationprob}
\begin{aligned}
	prob_i &= \max_{0 \leq j \leq m} \frac{BlockSize_{ji}}{reduceSize_i} \\
    m &= \text{number of map tasks}
\end{aligned}
\end{equation}
\hlyellow{
This parameter is used to achieve a better data locality while performing shuffle pre-scheduling. 
}

\begin{figure*}
	\centering
	\begin{subfigure}[b]{0.32\linewidth}
		\includegraphics[width=\linewidth]{fig/hash_pre}
		\caption{Linear Regression Prediction of Hash Partitioner}
		\label{fig:hash_pre}
	\end{subfigure}
	\begin{subfigure}[b]{0.32\linewidth}
		\includegraphics[width=\linewidth]{fig/range_pre_sample}
		\caption{Linear Regression and Sampling Prediction of Range Partitioner}
		\label{fig:range_pre_sample}
	\end{subfigure}
	\begin{subfigure}[b]{0.32\linewidth}
		\includegraphics[width=\linewidth]{fig/prediction_relative_error}
		\caption{Prediction Relative Error of Range Partitioner}
		\label{fig:prediction_relative_error}
	\end{subfigure}
	\caption{Reduction Distribution Prediction}
	\label{fig:dis}
	\vspace{-1em}
\end{figure*}

\subsubsection{Heuristic MinHeap Scheduling}\label{h-minheap}

\ifrevision
\marginnote{60D,E}
\fi

\hlyellow{
As long as the input sizes of reduce tasks are available, the pre-scheduling is a classic scheduling problem without considering the data locality. 
}
\hlyellow{
But as we mentioned before, the ignorance of data locality can introduce extra network transfer (e.g., Figure} \ref{fig:range_pre_sample}). 
\hlyellow{
In order to balance load on each node while reducing the network traffic, we present a heuristic MinHeap scheduling algorithm (Algorithm} \ref{hminheap}).   

\hlyellow{
For the pre-scheduling itself (i.e., the first $while$), the algorithm maintains a min-heap to simulate the load of each node 
and applies the longest processing time rule (LPT)} \cite{design}\hlyellow{ to achieve $4/3-approximation$ optimum. }
\hlyellow{
Since the size of tasks are considered while scheduling, Heuristic MinHeap can achieve shorter makespan than Spark FIFO which is a $2-approximation$ optimum. 
}
Simulation of OpenCloud trace in Figure \ref{fig:sim} also shows that Heuristic MinHeap has a better improvement (average 5.7\%) than the Spark FIFO (average 2.7\%).
% After the scheduling, the completion time of reduce stage is close to the optimal. \textcolor{red}{may need to add math prove between this and optimal}.
\hlyellow{
The task --- node mapping will be adjusted according to the locality after pre-scheduling. 
For a task which contains at most $prob$ data from $host\_id$, the $SWAP\_TASKS$ will be triggered when the $host\_id$ does not equal the $assigned\_id$.
% The closer $prob$ is to $1/m$, the more evenly this reduce partition is produced in cluster.
The normalized probability $norm$ is calculated as a bound of performance degradation. 
% We set maximum $upper\_bound$ of performance degradation equals to 10\% that can be traded for locality (in extreme skew scenarios).
Inside the $SWAP\_TASKS$, tasks will be selected and swapped without exceeding the $upper\_bound$ of each host. 
}

\begin{minipage}{\columnwidth}
\begin{algorithm}[H]
\caption{Heuristic MinHeap Scheduling for Single Shuffle}
\label{hminheap}
	\begin{algorithmic}[1]
	\small
	\Procedure{\Large schedule}{$m, host\_ids, p\_reduces$}
		\State $m\gets$ partition number of map tasks
		\State $R\gets$ sort $p\_reduces$ by size in non-increasing order
		\State $M\gets$ min-heap $\left\{ host\_id \rightarrow \left( \left[ reduces \right], size \right) \right\}$
		\State $idx\gets 0$
		\While{$idx <$ len$R$}
		% \Comment{Schedule reduces by MinHeap}
		\State $M\left[0\right].size \mathrel{+}= R\left[idx\right].size$
		\State $M\left[0\right].reduces.append\left(R\left[idx\right]\right)$
		\State $R\left[idx\right].assigned\_id \gets M \left[0\right].host\_id$
		\State Sift down $M\left[0\right]$ by $size$
		\State $idx\gets idx-1$
		\EndWhile
		\State $max\gets$ maximum size in $M$
		% \State Convert $M$ to mapping $\left\{ host\_id \rightarrow \left( \left[ rid\_arr \right], size \right) \right\}$
		\ForAll{$reduce$ in $R$}
		\Comment{Heuristic locality swap}
			\If{$reduce.assigned\_id \neq reduce.host\_id$}
				\State $p\gets reduce.prob$
				\State $norm\gets \left(p-1/m\right)/\left(1-1/m\right)/10$
				\State $upper\_bound \gets \left(1 + norm\right) \times max$
				\State SWAP\_TASKS$\left(M, reduce, upper\_bound\right)$
			\EndIf
		\EndFor
		% \Comment{$m$ is the number of input data}
		% \Comment{$r$ is partition number of reduces}
		% \Comment{$hosts$ is array of (hostid, partitionids[], size)}
		% \Comment{$c$ is $r*m$ array of composition data}
		% \Comment{$pSize$ is $r$ size array of predicted size of reduces}
		\Return $M$
	\EndProcedure
	\Procedure{\Large swap\_tasks}{$M, reduce, upper\_bound$}
		\State Swap tasks between node $host\_id$ and node $assigned\_id$
		\State of $reduce$ without exceeding the $upper\_bound$
		\State of both nodes.
		\State Return if it is impossible.
		% \State $reduces \gets M\left[reduce.host\_id\right].reduce$	
		% \State $candidates \gets$ Select from $reduces$ that $assigned\_id \neq host\_id$ \textbf{and} total size closest to $reduce.size$
		% \State $\Delta size \gets sizeOf\left(candidates\right) - reduce.size$
		% \State $size\_host \gets M\left[reduce.host\_id\right].size - \Delta size$
		% \State $size\_assigned \gets M\left[reduce.assigned\_id\right].size + \Delta size$
		% \If{$size\_host\leq upper\_bound$ \textbf{and} \\
		% 	\qquad \; $size\_assigned\leq upper\_bound$}
		% 	\State Swap $candidates$ and $reduce$
		% 	\State Update $size$ in $M$
		% 	\State Update $assigned\_host$ in $candidates$ and $reduce$
		% \EndIf
	\EndProcedure
	\end{algorithmic}
\end{algorithm}
\end{minipage}
\subsubsection{Cope with Multiple Shuffle Dependencies}
\ifrevision
\marginnote{60E}
\fi
\hlyellow{
A reduce stage can have more than one shuffle dependencies in the current DAG computing frameworks. 
}
The technique mentioned in Section \ref{shuffleprediction} can only handle an ongoing shuffle. 
For those pending shuffles, it is impossible to predict the sizes. 
This dilemma can be relieved by having all map tasks of different shuffles to be launched simultaneously. 
But doing this introduces large overhead such as extra task serialization. 
To avoid violating the optimization from framework, we present Accumulate Heuristic Scheduling algorithm to cope with multiple shuffle dependencies.

As illustrated in Algorithm \ref{mhminheap}, the sizes of previous $shuffles$ on each node are counted. 
\hlyellow{
When a new shuffle starts, the predicted $size$, $prob$, and $host\_id$ in $p\_reduces$ are accumulated with previous $shuffles$. 
After scheduling, if the new $assigned\_host$ of a reduce does not equal the original one, a re-shuffle will be triggered to transfer data to the new host for further computing. 
This re-shuffle is rare since the previous shuffle data contributes a huge composition (i.e., high $prob$) after the accumulation, 
which leads to a higher probability of tasks swap in $SWAP\_TASKS$. 
}
% It means the $SWAP\_TASKS$ can help revise the scheduling to match the previous mapping as much as possible while maintaining the good load balance.

% As for the input of $SCHEDULE$ function in Algorithm \ref{hminheap}, $m$ is the partition number of input data; $h$ is the array of nodes ID in cluster; $p\_reduces$ is the predicted reduce matrix. Each row in $p\_reduces$ contains $rid$ as reduce partition ID; $size$ as predicted size of this partition; $prob$ as the maximum composition portion of reduce data; $host\_id$ as the node ID that produces the maximum portion of reduce data, and $assigned\_id$ as the node ID assigned by Algorithm \ref{hminheap}. As for $M$, it is consists $host\_id$, an array of $reduce$ scheduled on this node and the $size$ of them.

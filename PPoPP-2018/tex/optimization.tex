\section{Shuffle Optimization}\label{opt}
This section presents the detailed methodologies to achieve three design goals. The out-of-framework shuffle data management is used to decouple shuffle from execution and provide a cross-framework optimization. Two heuristic algorithms (Algorithm \ref{hminheap}, \ref{mhminheap}) are used to achieve shuffle data pre-fetching without launching tasks.

\subsection{Decouple Shuffle from Execution}
During map tasks, the partitioner takes a set of key-value pairs as input and calculates the partition number for each of them by applying pre-defined the partition function. After that, it stores all key-value pairs in the corresponding data blocks. Each of the block contains the key-value pairs for one reduce partition. At the same time, blocks will be spilled to disk. In order to avoid blocking the slot by disk operation, we use memory copy to hijack shuffle data from map tasks. By doing this, a slot can be released as soon as a task finishes CPU intensive computing. 
From the perspective of reduce task, shuffle read is decoupled by pre-fetching shuffle data to local SCache client before reduce tasks start.

\begin{figure}
	\centering
	\includegraphics[width=0.75\linewidth]{fig/sim}
	\caption{Stage Completion Time Improvement of OpenCloud Trace}
	\label{fig:sim}
\end{figure}

\begin{figure}
	\centering
	\includegraphics[width=0.6\linewidth]{fig/reduce_cdf}
	\caption{Shuffle Time Fraction CDF of OpenCloud Trace}
	\label{fig:cdf}
\end{figure}
%To this end, all I/O operations are managed outside of the DAG framework, and the slot is occupied only by the CPU intensive phases of task.
\subsection{Pre-schedule with Application Context}
The pre-scheduling and pre-fetching start when the collected shuffle data exceeds the threshold.
This is the most critic step toward the optimization. The task --- node mapping is not determined until tasks are scheduled by the scheduler of DAG framework. Once the tasks are scheduled, the slots will be occupied to launch them. On the other hand, the shuffle data cannot be pre-fetched without the readiness of task --- node mapping.
To get rid of this dilemma, we propose a co-scheduling scheme with two heuristic algorithms (Algorithm \ref{hminheap}, \ref{mhminheap}). That is, the task --- node mapping is established ahead of DAG framework scheduler, and it is enforced to DAG scheduler before the real scheduling.

\begin{figure}
	\centering
	\includegraphics[width=0.9\linewidth]{fig/shuffle}
	\caption{Shuffle Data Prediction}
	\label{fig:shuffle}
	\vspace{-1em}
\end{figure}
% We explore several pre-scheduling schemes in different scenarios, and evaluate the performance calculating the improvement of reduce tasks completion time with trace of OpenCloud \cite{opencloudtrace}. We first emulate the scheduling algorithm of Spark to schedule the reduce tasks of one job, and take the bottleneck of the task set as the completion time. Then we remove the shuffle read time as the assumption of shuffle data pre-fetch and emulate under different schemes. The result is shown in \ref{fig:sim}.
% \begin{figure*}
% 	\centering
% 	\begin{minipage}{0.34\linewidth}
% 		\begin{figure}[H]
% 			\includegraphics[width=\textwidth]{fig/shuffle_size}
% 			\caption{Shuffle Size Comparing with Input Size}
% 			\label{fig:shuffle_size}
% 		\end{figure}
% 	\end{minipage}
% 	\begin{minipage}{0.65\linewidth}
% 		\begin{figure}[H]
% 			\begin{subfigure}{0.5\textwidth}
% 				\includegraphics[width=\linewidth]{fig/reduce_cdf}
% 				\caption{Shuffle Time Fraction CDF}
% 				\label{fig:cdf}
% 			\end{subfigure}	
% 			\begin{subfigure}{0.5\textwidth}
% 				\includegraphics[width=\linewidth]{fig/sim}
% 				\caption{Stage Completion Time Improvement}
% 				\label{fig:sim}
% 			\end{subfigure}	
% 			\caption{Emulate Result of OpenCloud Trace}
% 		\end{figure}
% 	\end{minipage}
% \end{figure*}
% \begin{figure}
\subsubsection{Problem of Random Mapping}\label{randomassign}
The simplest way of pre-scheduling is mapping tasks to different nodes randomly. It only guarantees that each node run same number of tasks. 
As shown in Figure \ref{fig:sim}, we use traces from OpenCloud \footnote{\label{fn:trace}http://ftp.pdl.cmu.edu/pub/datasets/hla/dataset.html} for the simulation to evaluate the impact of different pre-scheduling algorithms. The baseline (red dot line in Figure \ref{fig:sim}) is the stage completion time with Spark default scheduling algorithm. And then we remove the shuffle read time of each task, and run the simulation under three different schemes: random mapping, Spark FIFO, and our heuristic MinHeap.
Note that most of the traces from OpenCloud is shuffle-light workload as shown in Figure \ref{fig:cdf}. The average shuffle read time is 3.2\% of total reduce completion time.

Random mapping works well when there is only one round of tasks. But the performance collapses as the round number grows. It is because that data skew commonly exists in data-parallel computing \cite{skewtune, reining, gufler2012load}. Several heavy tasks might be assigned on the same node. This collision then slows down the whole stage and makes the performance even worse than the baseline. In addition, randomly assigned tasks also ignore the data locality between shuffle map output and shuffle reduce input, which might introduces extra network traffic in cluster.

\begin{figure*}
	\centering
	\begin{subfigure}[b]{0.32\linewidth}
		\includegraphics[width=\linewidth]{fig/hash_pre}
		\caption{Linear Regression Prediction of Hash Partitioner}
		\label{fig:hash_pre}
	\end{subfigure}
	\begin{subfigure}[b]{0.32\linewidth}
		\includegraphics[width=\linewidth]{fig/range_pre_sample}
		\caption{Linear Regression and Sampling Prediction of Range Partitioner}
		\label{fig:range_pre_sample}
	\end{subfigure}
	\begin{subfigure}[b]{0.32\linewidth}
		\includegraphics[width=\linewidth]{fig/prediction_relative_error}
		\caption{Prediction Relative Error of Range Partitioner}
		\label{fig:prediction_relative_error}
	\end{subfigure}
	\caption{Reduction Distribution Prediction}
	\label{fig:dis}
\end{figure*}

\subsubsection{Shuffle Output Prediction}\label{shuffleprediction}
The problem of random mapping was obviously caused by application context (e.g., shuffle data size) unawareness. Note that the optimal schedule decision can be made under the awareness of shuffle dependencies number, partition number, and shuffle size for each partition. The first two of them can be easily extracted from DAG information. The scheduling can be made with the "prediction" of shuffle size.

According to the DAG computing process, the shuffle size of each reduce task is decided by input data, map task computation, and hash partitioner. Each map task produces a data block for each reduce task. The size of each reduce partition can be calculated $reduceSize_i = \sum_{j=0}^{m} {BlockSize_{ji}}$ ($m$ is the number of map tasks). $BlockSize_{ji}$ represents the size of block which is produced by map $task_j$ for reduce $task_i$ (e.g., block `1-1' in Figure \ref{fig:shuffle}).

For the simple DAG applications such as Hadoop MapReduce \cite{mapreduce}, the $BlockSize_{ji}$ can be predicted with decent accuracy by liner regression model based on observation that the ratio of map output size and input size are invariant given the same job configuration \cite{ishuffle, predict}.

But the sophisticated DAG computing frameworks like Spark introduce more uncertainties. For instance, the customized partitioner might bring large inconsistency between observed map output blocks distribution and the final shuffle data distribution. In Figure \ref{fig:dis}, we use different datasets with different partitioners, and normalize the distribution to $0-1$ to fit in one figure. In Figure \ref{fig:hash_pre}, we use a random input dataset with the hash partitioner. In Figure \ref{fig:range_pre_sample}, we use a skew dataset with the range partitioner of Spark \cite{apachespark}.
The observed map outputs are randomly picked. As we can see, in hash partitioner, the distribution of observed map output is close to the final reduce input distribution. The prediction results also turn out to be good. However, this inconsistency results in a deviation in linear regression model.
% Several map outputs (marked as Map Output in Figure \ref{fig:shuffle}) are picked as observation objects to train the model and than predict the final reduce distribution.
To handle this inconsistency, we introduce another methodology named weighted reservoir sampling. The $BlockSize_{ji}$ can be calculated by
\begin{equation}
\label{equationsample}
\begin{aligned}
	BlockSize_{ji} &= {{InputSize_j \times \frac{sample_i}{s \times p}}} \\
	sample_i &= \text{number of samples for $reduce_i$}
\end{aligned}
\end{equation}
% The classic reservoir sampling is designed for randomly choosing \textit{k} samples from \textit{n} items, where \textit{n} is either a very large or an unknown number \cite{reservoir}. 
For each partition of map task, we use classic reservoir sampling to randomly pick $s \times p$ of samples, where $p$ is the number of reduce tasks and $s$ is a tunable number. After that, the map function is called locally to process the sampled data (\textit{sampling} in Figure \ref{fig:shuffle}). The final sampling outputs are collected with the $InputSize$ of each map partition which is used as the weight for each set of samples.

In Figure \ref{fig:range_pre_sample}, we set $s$ equals $3$, the result of sampling prediction is much better even in a very skew scenario. The variance of the normalization between sampling prediction and reduce distribution is because the standard deviation of the prediction result is relatively small compared to the average prediction size, which is $0.0015$ in this example. Figure \ref{fig:prediction_relative_error} further proves that the sampling prediction can provide precise result even in the dimension of absolute shuffle partition size. On the opposite, the result of linear regression comes out with large relative error.

To avoid extra overhead, the sampling prediction will be triggered only when the range partitioner or customized non-hash partitioner occurs. We will show the detail evaluation of sampling in the Section \ref{evaluation}.

During the phase of shuffle output prediction, the composition of each reduce partition is calculated as well. We define $prob_i$ as
\begin{equation}
\label{equationprob}
\begin{aligned}
	prob_i &= \max_{0 \leq j \leq m} \frac{BlockSize_{ji}}{reduceSize_i} \\
    m &= \text{number of map tasks}
\end{aligned}
\end{equation}
This parameter is used to achieve a better locality while performing shuffle scheduling.

\subsubsection{Heuristic MinHeap Scheduling}\label{h-minheap}
In order to balance load on each node while reducing the network traffic, we present a heuristic MinHeap scheduling algorithm (Algorithm \ref{hminheap}) for single shuffle.  
Heuristic MinHeap can be divided into two rounds. In the first round (i.e., the first $while$ in $SCHEDULE$), the reduce tasks are first sorted by size in a descending order. For hosts, we use a min-heap according to size of assigned tasks to maintain the priority. So that the tasks can be distributed evenly in the cluster.
% After the scheduling, the completion time of reduce stage is close to the optimal. \textcolor{red}{may need to add math prove between this and optimal}.
In the second round, the task --- node mapping will be adjusted according to the locality. 
The $SWAP\_TASKS$ will only be trigged when the $host\_id$ of a reduce task is not equal the $assigned\_id$.
% The closer $prob$ is to $1/m$, the more evenly this reduce partition is produced in cluster.
For a task which contains at most $prob$ data from $host$, the normalized probability $norm$ is calculated as a bound of performance degradation. We set maximum $upper\_bound$ of performance degradation equals to 10\% that can be traded for locality (in extreme skew scenarios).
Inside the $SWAP\_TASKS$, tasks will be selected and swapped without exceeding the $upper\_bound$ of each host. 
Combining these information helps the scheduler make a more balanced task --- node mapping than the na\"{i}ve Spark FIFO scheduling algorithm. 
We use the OpenCloud trace to evaluate Heuristic MinHeap. Without swapping, the Heuristic MinHeap can achieve a better performance improvement (average 5.7\%) than the default Spark FIFO scheduling algorithm (average 2.7\%).
This is the by-product optimization harvested from shuffle size prediction.

\subsubsection{Cope with Multiple Shuffles}
Multiple shuffles commonly exist in modern DAG computing. The techniques mentioned in Section \ref{shuffleprediction} can only handle the ongoing shuffle. For those pending shuffles, it is impossible to predict the sizes. This dilemma can be relieved by having all map tasks of shuffle to be scheduled by DAG framework simultaneously. But doing this introduces large overhead such as extra task serialization. To avoid violating the optimization from framework, we present Accumulate Heuristic Scheduling algorithm (Algorithm \ref{mhminheap}) to cope with multiple shuffles.

As illustrated in $M\_SCHEDULE$, the size of reduce on each node of previously scheduled $shuffles$ are counted. When a new shuffle starts, the $M\_SCHEDULE$ is called to schedule the new one accumulatively. The $size$ of each reduce and its corresponding $porb$ and $host$ in $p\_reduces$ are updated with data of $shuffles$ before $SCHEDULE$ is called. When the new task --- node mapping is available, if the new $assigned\_host$ of a reduce does not equal to the original one, the re-shuffle will be triggered to transfer data to a new host for further computing. This re-shuffle is rare since the previously shuffled data in a reduce partition contributes a huge composition. It means in the schedule phase, the $SWAP\_TASKS$ can help revise the scheduling to match the previous mapping as much as possible while maintaining the good load balance.

% As for the input of $SCHEDULE$ function in Algorithm \ref{hminheap}, $m$ is the partition number of input data; $h$ is the array of nodes ID in cluster; $p\_reduces$ is the predicted reduce matrix. Each row in $p\_reduces$ contains $rid$ as reduce partition ID; $size$ as predicted size of this partition; $prob$ as the maximum composition portion of reduce data; $host\_id$ as the node ID that produces the maximum portion of reduce data, and $assigned\_id$ as the node ID assigned by Algorithm \ref{hminheap}. As for $M$, it is consists $host\_id$, an array of $reduce$ scheduled on this node and the $size$ of them.

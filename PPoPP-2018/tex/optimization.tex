\section{Shuffle Optimization}
\label{opt}
This section presents the detailed methodologies to achieve three design goals. 
The out-of-framework shuffle data management is used to decouple shuffle from execution and provide a cross-framework optimization. 
Two heuristic algorithms (Algorithm \ref{hminheap}, \ref{mhminheap}) and \hlyellow{a co-scheduler} are used to achieve shuffle data pre-fetching without launching tasks.

\subsection{Decouple Shuffle from Execution}
To achieve the decouple from map tasks and reduce tasks, the original shuffle write and read implementation in the current frameworks should be modified to apply the API of SCache.
\ifrevision
\reversemarginpar
\marginnote{60B}
\fi
\hlyellow{
During shuffle write of map tasks, the partitioned shuffle data blocks will be written on the disks.
Each block contains a part of input data of a particular reduce task. 
To avoid the release of a slot being blocked by disk operation, SCache provides a disk-write-like API named $putBlock$ to handle the shuffle write.
Inside the $putBlock$, SCache uses memory copy to move the shuffle data blocks out of map tasks and store them in the reserved memory.
At the same time, the slot will be released immediately.
From the perspective of reduce task, SCache provides an API named $getBlock$ to replace the original implementation of shuffle read. 
With the precondition of shuffle data pre-fetching, 
the $getBlock$ leverages the memory copy to get the shuffle data requested by a reduce task from SCache.
Instead, the original implementation of shuffle read will trigger a network transfer that keeps CPU idle.
}

\begin{figure}
	\centering
	\includegraphics[width=0.6\linewidth]{fig/reduce_cdf}
	\caption{Shuffle Time Fraction CDF of OpenCloud Trace}
	\label{fig:cdf}
	\vspace{-1em}
\end{figure}

\begin{figure}
	\centering
	\includegraphics[width=0.75\linewidth]{fig/sim}
	\caption{Stage Completion Time Improvement of OpenCloud Trace}
	\label{fig:sim}
	\vspace{-1em}
\end{figure}

%To this end, all I/O operations are managed outside of the DAG framework, and the slot is occupied only by the CPU intensive phases of task.
\subsection{Pre-schedule with Application Context}
\hlyellow{
The pre-scheduling and pre-fetching the most critic steps toward the optimization. 
The task --- node mapping is not determined until tasks are scheduled by the scheduler of DAG framework. 
Once the tasks are scheduled, the slots will be occupied to launch them. 
On the other hand, the shuffle data cannot be pre-fetched without the awareness of task --- node mapping.
}
To get rid of this dilemma, we propose a co-scheduling scheme with two heuristic algorithms (Algorithm \ref{hminheap}, \ref{mhminheap}). 
\hlyellow{
That is, the task --- node mapping is established ahead of the task scheduling, and then it is enforced to the DAG scheduler when the task scheduling starts.
}

\begin{figure}
	\centering
	\includegraphics[width=0.9\linewidth]{fig/shuffle}
	\caption{Shuffle Data Prediction}
	\label{fig:shuffle}
	\vspace{-1em}
\end{figure}
% We explore several pre-scheduling schemes in different scenarios, and evaluate the performance calculating the improvement of reduce tasks completion time with trace of OpenCloud \cite{opencloudtrace}. We first emulate the scheduling algorithm of Spark to schedule the reduce tasks of one job, and take the bottleneck of the task set as the completion time. Then we remove the shuffle read time as the assumption of shuffle data pre-fetch and emulate under different schemes. The result is shown in \ref{fig:sim}.
% \begin{figure*}
% 	\centering
% 	\begin{minipage}{0.34\linewidth}
% 		\begin{figure}[H]
% 			\includegraphics[width=\textwidth]{fig/shuffle_size}
% 			\caption{Shuffle Size Comparing with Input Size}
% 			\label{fig:shuffle_size}
% 		\end{figure}
% 	\end{minipage}
% 	\begin{minipage}{0.65\linewidth}
% 		\begin{figure}[H]
% 			\begin{subfigure}{0.5\textwidth}
% 				\includegraphics[width=\linewidth]{fig/reduce_cdf}
% 				\caption{Shuffle Time Fraction CDF}
% 				\label{fig:cdf}
% 			\end{subfigure}	
% 			\begin{subfigure}{0.5\textwidth}
% 				\includegraphics[width=\linewidth]{fig/sim}
% 				\caption{Stage Completion Time Improvement}
% 				\label{fig:sim}
% 			\end{subfigure}	
% 			\caption{Emulate Result of OpenCloud Trace}
% 		\end{figure}
% 	\end{minipage}
% \end{figure*}
% \begin{figure}
\subsubsection{Problem of Random Mapping}\label{randomassign}
\hlyellow{
The simplest way of pre-scheduling is mapping tasks to different nodes randomly and evenly. 
In order to evaluate the effectiveness of random mapping, we use traces from OpenCloud \footnote{\label{fn:trace}http://ftp.pdl.cmu.edu/pub/datasets/hla/dataset.html} for the simulation.
}
Note that most of the traces from OpenCloud is shuffle-light workload as shown in Figure \ref{fig:cdf}. 
The average shuffle read time is 3.2\% of total reduce completion time.

As shown in Figure \ref{fig:sim}, the baseline (i.e., red dot line) is the stage completion time with Spark FIFO scheduling algorithm. 
And then we remove the shuffle read time of each task, and run the simulation under three different schemes: random mapping, Spark FIFO, and our heuristic MinHeap.
Random mapping works well when there is only one round of tasks. 
But the performance collapses as the round number grows. 
It is because that data skew commonly exists in data-parallel computing \cite{skewtune, reining, gufler2012load}. 
\hlyellow{
Several heavy tasks might be assigned on the same node, thus slowing down the whole stage. 
In addition, randomly assigned tasks also ignore the data locality between shuffle map output and shuffle reduce input, which might introduces extra network traffic in cluster.
}

\begin{figure*}
	\centering
	\begin{subfigure}[b]{0.32\linewidth}
		\includegraphics[width=\linewidth]{fig/hash_pre}
		\caption{Linear Regression Prediction of Hash Partitioner}
		\label{fig:hash_pre}
	\end{subfigure}
	\begin{subfigure}[b]{0.32\linewidth}
		\includegraphics[width=\linewidth]{fig/range_pre_sample}
		\caption{Linear Regression and Sampling Prediction of Range Partitioner}
		\label{fig:range_pre_sample}
	\end{subfigure}
	\begin{subfigure}[b]{0.32\linewidth}
		\includegraphics[width=\linewidth]{fig/prediction_relative_error}
		\caption{Prediction Relative Error of Range Partitioner}
		\label{fig:prediction_relative_error}
	\end{subfigure}
	\caption{Reduction Distribution Prediction}
	\label{fig:dis}
	\vspace{-1em}
\end{figure*}

\subsubsection{Shuffle Output Prediction}\label{shuffleprediction}
\ifrevision
\marginnote{60E}
\fi
\hlyellow{
The problem of random mapping was obviously caused by application context (e.g., shuffle data size) ignorance. 
Note that a balanced schedule decision can be made under the awareness of the number of shuffle dependencies, the number of shuffle data blocks, and the size of each block. 
The number of shuffle dependencies can be easily extracted from DAG information. 
The number of shuffle data blocks equals the number of map tasks $\times$ the number of reduce tasks, which can also be inferred from DAG.
So the pre-scheduling can be made if the "prediction" of size of shuffle block is practical.
}
% \hlyellow{
% The shuffle size of each reduce task is decided by input data, map task computation, and hash partitioner. 
% Each map task produces a data block for each reduce task. 
% }
The size of each reduce partition can be calculated $reduceSize_i = \sum_{j=0}^{m} {BlockSize_{ji}}$ ($m$ is the number of map tasks). 
$BlockSize_{ji}$ represents the size of block which is produced by map $task_j$ for reduce $task_i$ (e.g., block `1-1' in Figure \ref{fig:shuffle}).

For the most DAG applications with random large scale input, 
the $BlockSize_{ji}$ can be predicted with decent accuracy by liner regression model (i.e., equation \ref{linearregresion}) based on observation that the ratio of map output size and input size are invariant given the same job configuration \cite{ishuffle, predict}.
\begin{equation}
\label{linearregresion}
\begin{aligned}
	BlockSize_{ji} = k \times inputSize_j + b
\end{aligned}
\end{equation}
\hlyellow{
The $k$ and $b$ can be determined using the observed $inputSize_j$ of $j$th map task and $BlockSize_{ji}$.
}

\hlyellow{
Though the linear regression is stable in most scenarios, it can failed in some uncertainties introduced by the sophisticated frameworks like Spark} \cite{apachespark}.  
\hlyellow{
For instance, the customized partitioner might bring large inconsistency between observed map output blocks distribution and the final shuffle data distribution. 
}
\ifrevision
\reversemarginpar
\marginnote{60B}
\fi
We present two particular examples in Figure \ref{fig:hash_pre} and Figure \ref{fig:range_pre_sample}. 
In Figure \ref{fig:hash_pre}, we use a random input dataset with the hash partitioner. 
In Figure \ref{fig:range_pre_sample}, we use the data distribution of second that are partitioned by Spark RangePartitioner \cite{apachespark} shuffle of Spark Terasort \cite{spark-tera}. 
All of the numbers in both figures are normalized the distribution to $0-1$ to fit in one figure. 
% In Figure \ref{fig:hash_pre} and Figure \ref{fig:range_pre_sample}, we use different datasets with different partitioners, and normalize the distribution to $0-1$ to fit in one figure. 
The observed map outputs are randomly picked. 
As we can see, Figure \ref{fig:hash_pre}, the distribution of observed map output is close to the final reduce input distribution. 
The prediction results also turn out to be good. 
\hlyellow{
However, the inconsistency in Figure} \ref{fig:range_pre_sample} \hlyellow{results in a deviation in linear regression model. 
It is because the RangePartitioner leads to an extreme high locality skew. 
That is, for one reduce tasks, almost all of the input date are produced by a particular map task (e.g., the observed map only produces data for reduce tasks 0-5 in Figure} \ref{fig:fig:range_pre_sample}).

% Several map outputs (marked as Map Output in Figure \ref{fig:shuffle}) are picked as observation objects to train the model and than predict the final reduce distribution.
To handle this inconsistency, we introduce another methodology named weighted reservoir sampling. The $BlockSize_{ji}$ can be calculated by
\begin{equation}
\label{equationsample}
\begin{aligned}
	BlockSize_{ji} &= {{InputSize_j \times \frac{sample_i}{s \times p}}} \\
	sample_i &= \text{number of samples for $reduce_i$}
\end{aligned}
\end{equation}
% The classic reservoir sampling is designed for randomly choosing \textit{k} samples from \textit{n} items, where \textit{n} is either a very large or an unknown number \cite{reservoir}. 
For each partition of map task, we use classic reservoir sampling to randomly pick $s \times p$ of samples, where $p$ is the number of reduce tasks and $s$ is a tunable number. After that, the map function is called locally to process the sampled data (\textit{sampling} in Figure \ref{fig:shuffle}). The final sampling outputs are collected with the $InputSize$ of each map partition which is used as the weight for each set of samples.

In Figure \ref{fig:range_pre_sample}, we set $s$ equals $3$, the result of sampling prediction is much better even in a very skew scenario. The variance of the normalization between sampling prediction and reduce distribution is because the standard deviation of the prediction result is relatively small compared to the average prediction size, which is $0.0015$ in this example. Figure \ref{fig:prediction_relative_error} further proves that the sampling prediction can provide precise result even in the dimension of absolute shuffle partition size. On the opposite, the result of linear regression comes out with large relative error.

To avoid extra overhead, the sampling prediction will be triggered only when the range partitioner or customized non-hash partitioner occurs. We will show the detail evaluation of sampling in the Section \ref{evaluation}.

During the phase of shuffle output prediction, the composition of each reduce partition is calculated as well. We define $prob_i$ as
\begin{equation}
\label{equationprob}
\begin{aligned}
	prob_i &= \max_{0 \leq j \leq m} \frac{BlockSize_{ji}}{reduceSize_i} \\
    m &= \text{number of map tasks}
\end{aligned}
\end{equation}
This parameter is used to achieve a better locality while performing shuffle scheduling.

\subsubsection{Heuristic MinHeap Scheduling}\label{h-minheap}
In order to balance load on each node while reducing the network traffic, we present a heuristic MinHeap scheduling algorithm (Algorithm \ref{hminheap}) for single shuffle.  
Heuristic MinHeap can be divided into two rounds. In the first round (i.e., the first $while$ in $SCHEDULE$), the reduce tasks are first sorted by size in a descending order. For hosts, we use a min-heap according to size of assigned tasks to maintain the priority. So that the tasks can be distributed evenly in the cluster.
% After the scheduling, the completion time of reduce stage is close to the optimal. \textcolor{red}{may need to add math prove between this and optimal}.
In the second round, the task --- node mapping will be adjusted according to the locality. 
The $SWAP\_TASKS$ will only be trigged when the $host\_id$ of a reduce task is not equal the $assigned\_id$.
% The closer $prob$ is to $1/m$, the more evenly this reduce partition is produced in cluster.
For a task which contains at most $prob$ data from $host$, the normalized probability $norm$ is calculated as a bound of performance degradation. We set maximum $upper\_bound$ of performance degradation equals to 10\% that can be traded for locality (in extreme skew scenarios).
Inside the $SWAP\_TASKS$, tasks will be selected and swapped without exceeding the $upper\_bound$ of each host. 
Combining these information helps the scheduler make a more balanced task --- node mapping than the na\"{i}ve Spark FIFO scheduling algorithm. 
We use the OpenCloud trace to evaluate Heuristic MinHeap. Without swapping, the Heuristic MinHeap can achieve a better performance improvement (average 5.7\%) than the default Spark FIFO scheduling algorithm (average 2.7\%).
This is the by-product optimization harvested from shuffle size prediction.

\subsubsection{Cope with Multiple Shuffles}
Multiple shuffles commonly exist in modern DAG computing. The techniques mentioned in Section \ref{shuffleprediction} can only handle the ongoing shuffle. For those pending shuffles, it is impossible to predict the sizes. This dilemma can be relieved by having all map tasks of shuffle to be scheduled by DAG framework simultaneously. But doing this introduces large overhead such as extra task serialization. To avoid violating the optimization from framework, we present Accumulate Heuristic Scheduling algorithm (Algorithm \ref{mhminheap}) to cope with multiple shuffles.

As illustrated in $M\_SCHEDULE$, the size of reduce on each node of previously scheduled $shuffles$ are counted. When a new shuffle starts, the $M\_SCHEDULE$ is called to schedule the new one accumulatively. The $size$ of each reduce and its corresponding $porb$ and $host$ in $p\_reduces$ are updated with data of $shuffles$ before $SCHEDULE$ is called. When the new task --- node mapping is available, if the new $assigned\_host$ of a reduce does not equal to the original one, the re-shuffle will be triggered to transfer data to a new host for further computing. This re-shuffle is rare since the previously shuffled data in a reduce partition contributes a huge composition. It means in the schedule phase, the $SWAP\_TASKS$ can help revise the scheduling to match the previous mapping as much as possible while maintaining the good load balance.

% As for the input of $SCHEDULE$ function in Algorithm \ref{hminheap}, $m$ is the partition number of input data; $h$ is the array of nodes ID in cluster; $p\_reduces$ is the predicted reduce matrix. Each row in $p\_reduces$ contains $rid$ as reduce partition ID; $size$ as predicted size of this partition; $prob$ as the maximum composition portion of reduce data; $host\_id$ as the node ID that produces the maximum portion of reduce data, and $assigned\_id$ as the node ID assigned by Algorithm \ref{hminheap}. As for $M$, it is consists $host\_id$, an array of $reduce$ scheduled on this node and the $size$ of them.

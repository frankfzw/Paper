\documentclass[10pt,twocolumn]{article}
\usepackage{times}
\usepackage{url}

% do not change these values
\baselineskip 12pt
\textheight 9in
\textwidth 6.5in
\oddsidemargin 0in
\topmargin 0in
\headheight 0in
\headsep 0in

\begin{document}

\title{Title}
\author{Zhouwang Fu$^1$ and Zhengwei Qi$^1$ \\
% \small {\em  $^1$Shanghai Jiao Tong University \quad
%           $^2$Cloud National Labs} \\ [2mm]
\small {\em  $^1$Shanghai Jiao Tong University} \\ [2mm]
% \small Submission Type: Research
}
\date{}
\maketitle

\begin{abstract}
Shuffle is the term used to descirbe the cross-network read and aggregation
of partitioned ancestor data before invoking reduce operation.
As DAG computing framworks keep evolving, calculation and scheduling of each task are well optimized. 
However shuffle cuts off the data processing pipeline, introduce significant latency to successors.
To remove shuffle overhead, we present XXX, a plugin system to decouple shuffle from DAG computing 
framework. XXX captures shuffle data in the memory and uses heuristic-FIFO scheduling to balance data
blocks to elimite the explict barrier. We implement XXX and change Spark to use XXX as external shuffle 
 service and scheduler. We evaluate XXX performance both on simulation and 50-machine Amazon EC2 cluster.
 Results show that, by incorporating XXX in Spark, the shuffle overhead can be reduce XXX.

\end{abstract}

\section{Introduction}

\section{Motivation}
In this section, we first study the shuffle pattern (\ref{shuffle pattern}). 
Then we show the observation of the opportunities to optimize shuffle in \ref{observation}
\subsection{Characteristic of Shuffle} \label{shuffle pattern}
In DAG computing model such as Hadoop \cite{hadoop} and Spark \cite{spark}, shuffle is dedicated to achieve a
all-to-all data tranfer between stages.

It's I/O intensive, not CPU intensive. Write to disk for fault tolerance. Use network to transfer data. 

It loosely couple with application context. Only blocks of bytes.
\subsection{Observation} \label{observation}
0. DAG computing have multi-rounds taks. Better load balance. Fit data blocks into main memory. 
Example Hadoop MapReduce suggests 10-100 maps per-nodes and 0.95 to 1.75 multiplied by no.node * no.container per node.\cite{hadooptutorial}

1. Disk is not necessary for fault tolerance. Memory is very fast, Network can be fast. Example SSD 480 MB/S max. 
For 10Gbps network, 3X faster than SSD.

2. It's not efficient to put shuffle read/write operation into a task. Task is a unit of computation. 
But shuffle doesn't involve CPU

3. Shuffle can be easly decoupled from task.
Based on these observations, it's straightforward to come up with a optimization to overlap the I/O operation of shuffle
by leveraging multi-rounds property of DAG computing. In order to achieve this optimization, we have to decouple shuffle and 
perform pre-fetch as soon as each output of ancestor task is available. But is this feasible? We try to answer this question
in the following sections.

\section{Achieve Shuffle Pre-fetch}
\subsection{Decouple shuffle from task}
Instead of writing shuffle data into disk, the data can be intercepted in memory and directly writen to network.
But where?
\subsection{Pre-schedule with Application Context}
Naively random map tasks to end hosts. It's bad, may skew. We have perform more balanced allocation to avoid hurting the performance of successor tasks.
Show the simulation with open cloud trace. 

In order to acheive better balanced allocation, we should combine with application context and DAG.

For one shuffle, we can use first few tasks output to predict reduce distribution such as \cite{ishuffle}. 

But results vary due to the input data distribution and partition function. Show three pics (hash parition and two range partition)

For hash partition function, in most scenrios are enough to have first completed tasks.

For range partition function and customed partition function, the relation between one output and the whole distribution can be oppsite.
To avoid complex modification, we keep the partition function as a black box and use weighted reservior sampling to prob the distribution.
Show the prediction result and accurance.

For each prediction result, a percentage array of total data composition is calculated.

Combined with DAG information, i.e. other co-existing shuffle dependencies.

present pseudo code. (not completed yet).

\section{Design}
\section{Evaluation}

\section{Conclusion}



\bibliographystyle{abbrv}
\bibliography{biblio}

\end{document}



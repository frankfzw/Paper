%# -*- coding: utf-8-unix -*-
%%==================================================
%% chapter02.tex for SJTU Master Thesis
%%==================================================

\chapter{SCache的实现}
\label{chap:impl}

本章将展现SCache系统的实现概况。
SCache是一个开源的shuffle数据管理系统，并且提供了一个对于DAG任务预调度的附属调度器。
同时SCache还在设计时提供了跨框架的接口，来实现对于现有主流分布式DAG计算框架的shuffle优化。
在这次实现中，我们以Spark作为DAG计算框架的实例来阐述在SCache辅助下DAG的新的计算流程。
我们首先在章节\ref{sec:overview}中介绍了系统设计的概要。
之后的两个章节主要介绍应用SCache优化上工程上的开销和SCache在容错性上的取舍。

\section{系统设计概要}
\label{sec:overview}

SCache在系统层面上主要包含三个部件：一个分布式的shuffle数据管理系统，一个DAG的附属调度器，和一个Spark系统内部的守护进程。
如图\ref{fig:arch}所示，SCache采用了类似于GFS\cite{gfs}经典的主从节点架构来实现对shuffle数据在集群中的管理。

\begin{figure}[!htp]
	\centering
	\includegraphics[width=0.6\textwidth]{../../PPoPP-2018/fig/arch.pdf}
	\bicaption[fig:arch]{SCache系统架构示意图}{SCache系统架构示意图}{Fig}{SCache Architecture}
\end{figure}

SCache的主节点负责通过DAG的附属调度器获取Spark上连接shuffle的reduce阶段任务预调度信息，任务的调度执行顺序等。SCache的主节点会根据reduce阶段任务的预调度信息通知到各个从属工作节点。
当工作节点收到任务预调度信息之后，会将本地已经缓存的map任务输出的shuffle数据分别发送到目的节点。
并且对于未完成的map任务，一旦工作节点通过内存拷贝的方式获取了相应的shuffle数据后就立即通过网络将shuffle数据分发出去。
除此之外，SCache主节点还会根据任务的执行顺序信息给各个shuffle的数据单元标注上优先级并且发送给各个工作节点。

SCache的从节点会在本地占用一部分内存空间用来存储shuffle的数据块。
为了减少内存管理的开销，SCache使用了Java虚拟机的堆外内存来对shuffle的数据块进行存储。
采用这种方式，既可以减少序列化反序列化带来的计算资源开销，同时又减轻了Java虚拟机垃圾回收算法（Garbage Collection）中的计算复杂度，有利于提升系统的整体性能。

于此同时，当本地的内存空间不够时，各个工作节点会根据从主节点收到的shuffle数据块优先级信息，结合本地的缓存信息（比如是否存在不完整的shuffle存储单元）来将一部分shuffle数据先保存到磁盘上。
本地的工作节点会在内存中至少缓存一个优先级最高的reduce任务需要的shuffle数据。
当本地工作节点的shuffle缓存数据被任务消耗时，该部分内存空间就会被释放，而在磁盘中缓存的较高优先级的数据就会被立刻放入内存中。
通过结合主节点的优先级信息，本地shuffle的缓存状况以及Spark任务对shuffle数据的访问状况，工作节点的调度可以保证相对独立的完成内存管理并且保证：（1）shuffle数据可以在reduce任务开始执行前就被缓存在内存当中并且（2）shuffle数据的内存缓存不会破坏全部或没有以及应用优先级的限制。

SCache中的DAG附属调度器主要负责从Spark的Driver中获取DAG的信息，包括map阶段和reduce阶段中的shuffle数目，map任务的个数和reduce任务的个数以及当前map任务的shuffle输出的数据分别或者采样任务之后的数据分布。
附属调度器会根据以上信息采用相应的线性回归算法或者采样算法来预测shuffle数据的分布，同时调用算法\ref{mhminheap}和算法\ref{hminheap}来作出一个启发式的调度。
在获得最终调度结果之后，附属调度器会将调度结果发送给SCache的主节点。
同时该调度结果也会在Spark任务调度器调度reduce阶段的任务之前将预调度的结果强制到任务调度器上。

守护进程以一个独立的线程的形式存在于Spark的内存空间，通过RPC的方式与SCache进行通信。
并且向Spark系统内部的守护进程则负责向Spark的任务和Driver提供相应的API。

\begin{table}[!hpb]
    \centering
    \bicaption[tab:apis]{SCache编程接口列表}{SCache编程接口列表}{Table}{API list of SCache}
    \begin{tabular}{ | m{2.5cm} | m{8cm} | m{5cm} | }
        \hline
        接口 & 参数 & 作用 \\ [0.5ex]
        \hline
        \hline
        registerShuffles & jobId: Int, shuffleIds: Array[Int], maps: Array[Int], reduces: Array[Int], partitioner: Array[String] & 向SCache注册shuffle \\ \hline
        getBlock & blockId: String & 向SCache获取shuffle的数据块 \\ \hline
        putBlock & blockId: String, data: Array[Byte], len: Int & 向SCache发送shuffle数据块 \\ \hline
        getShuffleStatus & jobId: Int, shuffleId: Int & 向SCache获取reduce任务的预调度结果 \\
        \hline
    \end{tabular}
\end{table}

\section{工作流程}

接下来我们将介绍在SCache的协同下Spark执行DAG的工作流程。
当一个Spark的工作启动时，首先会根据用户的代码生成一个关于RDD（Resilient Distributed Datasets）的系带关系（lineage）。
之后Spark的调度器会从最终的用户RDD递归向前寻找依赖的RDD。
在RDD之间的数据依赖中，如果存在部分依赖，也就是shuffle依赖，Spark会在此处插入一个shuffle过程，并且将之前的所有RDD合并成一个计算阶段（stage）。
递归寻找的过程会在当一个RDD的数据已经被计算或者已经到了存储系统的部分就会停止。
而这些计算阶段则最终组成了计算过程中的DAG逻辑。

对于DAG中相邻计算阶段之间的shuffle依赖，它们会被打包成一个RPC的调用提交到SCache的从属调度器上。如表\ref{apis}中，一个shuffle依赖需要包含一个唯一的整数ID代表该工作，同时需要包含这个shuffle依赖中每个shuffle的ID，以及它们对应的分区函数的类型，map阶段任务数和reduce阶段的任务数。
在收到一次RPC提交之后，SCache的从属调度器会首先检查分区函数的类型，如果不是哈希分区函数，就会通过在Spark的Driver上的守护进程在Spark执行该计算阶段前插入一段采样程序。
我们会在章节\ref{sec:sampling}中详细阐述这个过程。

对于一个哈希分区函数的map任务，当计算结束之后分区函数会将计算产生的键值对通过相应的哈希函数划分成不同的数据块。
对于每一个数据块，SCache在Spark系统中的daemon程序都会根据其所在的工作ID对应的shuffle依赖ID，map任务ID，和reduce任务ID对每个数据块进行一个唯一的编号（即\ref{tab:apis}中的blockId）。
在此之后daemon程序会首先通过远程过程调用（Remote Procedure Call）的方式将这些shuffle数据块的元数据发送给SCache本地节点的工作进程。
于此同时，daemon进程也会通过Java对象的序列化的方式，将相应的数据序列化成字节码。
当接受到元数据之后，SCache的本地进程就会通过内存拷贝的形式，将序列化之后的字节码数据从Spark执行器的Java虚拟机的内存空间通过daemon程序拷贝到外部SCache的内存空间。
一旦内存拷贝结束，该map任务所占用的计算资源（slot）就会被立即释放。
通过SCache此处的内存拷贝优化，使得原先阻塞的磁盘I/O操作被避免，从而实现了map任务端更细粒度的硬件资源管理，提高了硬件资源的利用率和复用率。

在SCache的本地进程获取了该map任务输出的shuffle数据块之后，它会通过SCache系统内部的RPC接口通知SCache的主节点，并且在通知中附上了该map任务对于所有reduce任务产生的shuffle数据块大小（如图\ref{fig:shuffle}中的“map output”）。
每个数据块的大小可以通过内存拷贝时每个“blockId”对应的字节数组的长度来获得（即接口\ref{tab:apis}中putBlock的参数data: Array[Byte]）。
当SCache的主节点收到了足够多的shuffle数据块信息之后（该threshold可以通过配置文件更改），会通过线性回归模型首先对reduce任务的输入数据分布进行预测。
之后DAG从属调度模块会根据预测数据，调用算法\ref{mhminheap}和算法\ref{hminheap}来对redcue任务进行预调度。
当预调度算法运行结束之后，SCache的主节点会将reduce任务与计算节点的映射关系进行广播。
每个工作节点的SCache进程收到预调度的信息之后，会根据节点信息做出筛选，选出需要在本地执行的reduce任务。
此后本地节点会根据相应的工作ID，shuffle依赖ID，map任务ID，以及即将在本地执行的reduce任务ID组合而成的blockId，查询获取该数据块所在的节点位置，之后便开始从该远程节点预取数据。
当一个数据块被预取之后，其在节点的内存缓存就会被释放。

在reduce任务被调度的阶段，为了使Spark任务调度器遵循SCache的预调度结果，我们修改Spark任务调度器，通过表\ref{tab:apis}中getShuffleStatus的接口获取SCache预调度的任务--节点映射关系。
在此基础上，我们将reduce任务中分配的节点修改成预调度的结果，同时对每个任务对节点的优先级设置为$NODE\_LOCAL$\cite{sparksource}来使得该任务被强制分配到预调度的节点，从而获取shuffle数据预取与内存缓存的优化。

\section{水塘抽样}
\label{sec:sampling}

在上文的工作流程中，如果在注册shuffle时SCache检测到不是哈希分区函数，则SCache部署在Spark主节点上的守护进程会在使用非哈希分区函数的RDD中调用其$mapPartitionsWithIndex$\cite{sparksource}方法对每个分区的数据进行水塘采样。
如图\ref{fig:sample}所示，对于每个计算节点的本地采样程序，我们使用了经典的水塘采样算法\cite{reservoir}，对每个数据分区中的输入数据进行随机采样计算并且统计经过分区函数分区后的输出shuffle数据分布。
在具体实现中，对于采样的样本数，我们将其设置为$3 \times $数据块数目，以此来平衡采样的精确度和采样的开销。
此处采样的样本数可以根据用户配置进行调整。
在对任务数据进行采样的同时，算法也会统计该数据分区的大小，并以此作为该任务分区采样数据的权重。
最终这些数据会在SCache主节点进行汇总，并根据公式\ref{eq:sample}来得出每个map任务对应每个
reduce任务所产生的数据块大小，之后主节点会根据reduce的任务ID进行汇总，最终得出对于下一轮reduce任务输入数据体积分布的预测。
获取预测数据之后，SCache主节点会同上文的工作流程中一致，即调用算法\ref{mhminheap}和算法\ref{hminheap}来进行任务预调度以及后续的shuffle数据预取等工作。

\begin{figure}[!htp]
	\centering
	\includegraphics[width=0.6\textwidth]{../../PPoPP-2018/fig/sample.pdf}
	\bicaption[fig:sample]{对于一个任务数据分区的采样}{对于一个任务数据分区的采样}{Fig}{Reservoir Sampling of One Data Partition}
\end{figure}

\section{容错机制}

本小结将从两个角度来讨论SCache在容错性上的设计：Spark执行器或者SCache进程的崩溃造成的错误以及节点崩溃造成的节点下线的错误。

对于Spark执行器发生的错误，SCache采用了独立于Spark执行器的内存空间进行shuffle数据的存储和管理，因此即使Spark的执行器因为错误或者调度原因被Spark主节点杀死或者重启，其中所执行的任务需要的shuffle数据仍然不会遗失。

对于SCache进程的容错性，SCache采用了本地磁盘备份的机制来防止因为错误造成的shuffle数据损失。
当shuffle数据块预取的同时，无论该数据块是否在内存中有缓存，SCache都会在节点本地通过异步I/O的方式在本地磁盘对该数据块进行备份。
这些备份会在shuffle数据块被任务使用之后释放。
同时，SCache的工作节点会定期向主节点发送心跳信息，一旦主节点没有收到其中一个工作节点的心跳，就可以通过远程脚本控制其在本地重启。
当工作节点重启之后，主节点会向其发送当前shuffle的调度状态，工作节点则会根据本地磁盘的shuffle数据块备份状况和当前的shuffle状态来判断是否有未完成的shuffle数据块预取以及缓存任务。
而在工作节点进程失效的过程中，Spark的执行器如果有相应的reduce任务执行需要数据，则会被其中的守护进程阻塞住，直到本地工作进程完成重启。
对于SCache主节点的容错性，由于主节点的进程只服务与一个Spark的driver程序，并且不需要调度对于跨工作的shuffle数据依赖，因此主节点采用了简单的本地磁盘备份日志的方式来备份当前对于相关shuffle的元数据记录和调度记录等信息。

为了降低系统复杂度，SCache在设计的过程中并未引入应对节点崩溃下线的容错机制。
省略这部分机制的原因主要有一下几点：
\begin{itemize}
    \item 备份机制的开销过大，与优化shuffle过程的性能目标相对立。在设计系统的过程中我们并未加入比如经典的GFS\cite{gfs}中提出的三备份机制。
    采用此类容错机制虽然能提升系统的鲁棒性，但是同时也会增加系统在执行shuffle过程中的开销。
    比如如果采用三备份机制，那么在shuffle数据预取阶段以及本地shuffle存储的开销都会随着备份数目的增加而倍增。
    虽然在章节\ref{subsec:size}中我们发现shuffle数据体积对较小，但是备份机制带来的额外开销无疑会给数据中心网络的性能甚至内存和硬盘的性能带来一定挑战，这也与我们的优化目标相违背。
    \item 需要备份的数据没有复用价值。对于每一个shuffle依赖中的数据，只会在DAG计算过程中被使用一次，之后便会被释放。因此对于这些数据的备份就不像一些复用性较强的存储系统甚至Spark本身的RDD来的重要。
    \item SCache与计算框架的共生性。为了提高效率，SCache采用了与DAG计算框架一一对应的共生设计，即每个计算节点既有DAG计算框架的执行器，又有SCache的工作进程。
    这种共生的模式也意味着当该节点失效时，SCache的工作进程和DAG的计算执行器会同时失效。而DAG计算框架本身又有不同的容错机制来保证任务执行。
    而此时对节点进行备份机制的设计可能不但不利于计算的快速恢复，反而会因为不同的容错策略导致SCache针对节点的荣作机制变成无效操作。
    比如在Spark中，会采取对失效的RDD分区进行并行恢复的模式，在此过程中原来属于一个数据分区的数据会进行再分区，从而加快恢复速度。
    那么此时针对SCache单节点的容错机制即使恢复了丢失的shuffle数据，该份数据在计算中也没有使用价值。
\end{itemize}

虽然在设计中省略了对节点下线的容错性，但是为了保证DAG计算过程在发生错误时仍然能够正确执行，SCache在此处采用了借助DAG计算框架容错性的恢复模式。
比如在上文提到的Spark恢复模式中，该失效的RDD分区会进行一个重新分区的并行计算，而在次过程中，SCache对于shuffle的优化将对这部分逻辑进行重新提交和分配。
通过借助DAG计算框架本身的容错性，SCache能保证在节点下线的恢复过程中不破坏任务的正确性，同时提供对恢复中的shuffle优化。



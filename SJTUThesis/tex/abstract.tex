%# -*- coding: utf-8-unix -*-
%%==================================================
%% abstract.tex for SJTU Master Thesis
%%==================================================

\begin{abstract}

大数据时代的到来使得分布式计算变得越来越普及。
为了快速地处理大规模的数据，有大量复杂的分布式并行计算框架被设计并使用，比如Hadoop MapReduce\cite{hadoop}，Spark\cite{apachespark}，Dryad\cite{dryad}, Tez\cite{tez}等。
这些分布式计算框架大多采用将用户计算逻辑用有向无环图(Directed Acyclic Graph, DAG)的方式呈现出来。
在执行DAG的每一个计算阶段时，这些计算框架大多采用了整体同步并行计算模型(Bulk-Synchronous Parallel, BSP)来对大数据进行分布式的并行批处理。

在这些相邻的计算阶段之间，shuffle，或者说跨网络的多对多分块数据的读写满足了计算逻辑对于不同数据的依赖。与此同时，shuffle的过程也带来了大量的网络数据传输。
受限于计算任务对于数据的依赖和本身低效率的设计实现，shuffle过程会给计算任务的性能带来巨大损失。
尤其是在一些需要大量shuffle数据的情境中，shuffle的开销甚至会成为整个应用的性能瓶颈。
更重要的是，这个问题在大多数分布式并行计算框架中都普遍存在。

为了提供一种具有普遍意义的shuffle优化方案，本研究抽取了这些系统在shuffle设计中存在的一些共性问题：1)粗粒度的硬件资源管理降低了资源的利用率和复用率。
2)同步滞后的shuffle读取既增加了计算任务执行时对shuffle网络传输的显式等待时间，又给网络带来一个瞬时的流量高峰。

针对以上问题，本文提出了S(huffle)Cache --- 一个开源的即用型系统来优化DAG计算过程中的shuffle阶段。
通过在计算阶段真正执行前提取表达计算逻辑的DAG以及其中的shuffle依赖关系，SCache可以将shuffle过程从DAG计算过程中独立出来，从而提供更细粒度的硬件资源管理。
与此同时，SCache通过提前异步的shuffle传输来解决目前同步滞后的shuffle读取过程。
此外，SCache还利用内存来实现对shuffle数据的缓存，进一步提升shuffle过程的效率。
为了实现以上的优化目标，本研究做出了以下主要贡献：

\begin{enumerate}
    \item 将shuffle过程从计算过程中解耦，使得shuffle过程独立到外部进行管理，从而实现了更细粒度的硬件资源管理。
    \item 结合应用的上下文对shuffle数据进行预取，既避免了同步数据读取给网络带来的压力，又能将大部分网络传输时间隐藏到计算阶段。
    \item 结合应用的上下文对shuffle数据进行内存缓存，进一步提升shuffle过程的效率。
    \item 根据现有的分布式计算框架shuffle的特点设计了相应的接口(API)。通用的接口设计使得优化能被应用到不同的分布式并行计算框架当中。
\end{enumerate}

基于以上阐述，本研究课题实现了SCache，同时修改了Apache Spark\cite{apachespark}对SCache进行适配。
并且通过仿真实验和Amazon AWS EC2集群上大规模数据测试来验证其优化效果。
在不同的数据集和测试程序的测试中，SCache能减少将近89\%的shuffle开销。
在TPC-DS的测试中，SCache的优化能给分布式SQL查询带来平均大约40\%的性能提升。

\keywords{\large 分布式DAG计算框架， Shuffle， 优化}
\end{abstract}

\begin{englishabstract}

Recent years have witnessed the widespread use of distributed computing in the big data area.
Numbers of sophisticated distributed data parallel computing frameworks, such as Hadoop MapReduce\cite{hadoop}, Spark\cite{apachespark}, Dryad\cite{dryad}, and Tez\cite{tez},
have been developed and deployed to accelerate the big data processing.
Most of these frameworks do the computing by transforming the application logic into Directed Acyclic Graph (DAG).
In order to increase the parallelism, each computing stage is usually managed according to the Bulk-Synchronous Parallel (BSP) model during the execution of DAG.

Shuffle, or the cross-network read and aggregation of partitioned data among tasks with data dependencies in the consecutive execution stages, 
usually brings in large network transfer. 
Due to the dependency constrains and the limited performance of disks and networks, execution of those descendant tasks could be delayed by inefficient shuffles. 
This delay can further slow down the whole application process. 
The performance degradation introduced by shuffle can become overwhelming in the shuffle intensive applications.
Moreover, the above deficiencies of shuffle generally exist in most of the DAG data parallel computing frameworks. 
In this paper, we extract the common issues in current shuffle mechanism: 
1) The coarse granularity resource management decreases the utilization and multiplexing of hardware resources.
2）The synchronized shuffle read increases the explicit network waiting time during task execution and brings a network burst which further slows down the shuffle read itself.

Based on the above observations, we present S(huffle)Cache --- an open source plug-in system that particularly focuses on shuffle optimization in frameworks defining jobs as DAGs. 
By extracting and analyzing the DAGs and shuffle dependencies prior to the actual task execution, 
SCache can take full advantage of the fine granularity resource management and system memory to accelerate the shuffle process. 
Meanwhile, SCache manages the shuffle data out of the frameworks and transfers data asynchronously, which helps overlap the network transfer time and avoid network burst.
In addition, SCache provides an application-context-aware in-memory shuffle data management scheme to further accelerate the shuffle process.  
In order to achieve the optimizations, we make following contributions:

\begin{enumerate}
    \item Decouple the shuffles and manage them out of the DAG data parallel computing frameworks so that the shuffle data management can become more efficient.
    \item Implement the shuffle data pre-fetch with application context so that the network burst can be avoided and the network transfer time can be overlapped in execution phases.
    \item Implement the application-context-aware in-memory shuffle data management to accelerate the shuffle process.
    \item Design and implement the general APIs for the DAG data parallel computing frameworks so that the optimizations can be applied easily.
\end{enumerate}

We have implemented SCache and customized Apache Spark\cite{apachespark} to use it as the external shuffle service and co-scheduler. 
The performance of SCache is evaluated with both simulations and testbed experiments on a 50-node Amazon EC2 cluster.
Those evaluations have demonstrated that, by incorporating SCache, the shuffle overhead of Spark can be reduced by nearly 89\%, 
and the overall completion time of TPC-DS queries improves 40\% on average.


\englishkeywords{\large Distributed DAG frameworks, Shuffle, Optimization}
\end{englishabstract}


%# -*- coding: utf-8-unix -*-
%%==================================================
%% chapter01.tex for SJTU Master Thesis
%%==================================================

%\bibliographystyle{sjtu2}%[此处用于每章都生产参考文献]
\chapter{绪论}
\label{chap:intro}

大数据时代的到来和云计算的迅猛发展使得用分布式计算的方式来处理海量数据的方案变得水到渠成。
分布式并行计算框架的出现，特别是基于有向无环图（DAG）的方式来表达计算逻辑的分布式计算框架，进一步简化了大数据处理的过程。
也正是如此，使得分布式计算框架在短时间内获得了大量的普及。
虽然如何使得分布式计算变得更高效一直是近几年的研究热点，但是大量工作都集中在优化计算阶段的方向。
而对于其中shuffle阶段则关注较少。但是不能忽视的是，在许多场景下，shuffle这种I/O密集型的操作甚至会成为整个分布式计算应用的性能瓶颈。
本文针对现有的基于DAG的分布式计算框架的shuffle特点，提出了一种通用，高效的shuffle优化方案。
本文首先会介绍基于DAG分布式计算框架以及其优化的研究背景，然后阐述本文的优化目标以及国内外相关研究现状，最后简单介绍本文的结构组成。

\section{研究背景}

大数据时代的到来使得企业要处理的数据量远远超过了一台机器的处理性能。
在分布式计算普及之前，企业只能通过不断升级昂贵的超级计算机的性能来满足指数级增长的数据量。
然而随着Google公开了MapReduce\cite{mapreduce}的并行计算模式之后，分布式并行计算逐渐进入了蓬勃发展的阶段。
相对于昂贵而复杂的大型机，分布式计算能使用造价较低的商用机，并通过网络组合成集群，从而提供与超级计算机相匹配的运算能力来对大数据进行批处理。
最近几年，更是有大量的分布式计算框架在学术界和工业界得以发表和公开同时也有大量的计算框架被部署到企业的生产环境中，成为大数据生态系统中最重要的一个组件。
其中应用最广泛的就是Hadoop MapReduce\cite{mapreduce}，Spark\cite{apachespark}和Tez\cite{tez}等。

虽然这些计算框架的设计和实现大相径庭，但是究其本质上对任务进行分布式并行处理的基本理念还是有着许多相似之处。
本节将首先介绍目前业界应用最广泛的以及学术界最具代表性分布式计算过框架的工作流程。
之后分析概况和归纳其在执行分布式任务中存在共性的数据交互模式，以此来说明本课题的研究存在普遍意义。

这其中最经典的分布式计算框架便是Hadoop MapReduce\cite{hadoop}。
其运行过程主要通过有用户指定具体的mapper和reducer来实现分布式并行计算。
MapReduce对于用户指定的mapper和reducer会在执行过程中创建多个任务副本（Task）。
这些任务在任务调度器和资源调度器共同的调度下，会被分配到集群中的某个节点上的Java虚拟机（JVM）中进行执行。
而整个的MapReduce工作就是由这些mapper和reducer以及它们所对应的任务连接起来，行程一个流水线状的执行流程图。
具体可以参考图\ref{fig:mrdag}

\begin{figure}[!htp]
    \centering
    \includegraphics[width=0.8\textwidth]{mrdag.pdf}
	\bicaption[fig:mrdag]{Hadoop MapReduce执行示意图}{Hadoop MapReduce执行示意图}{Fig}{Hadoop MapReduce Execution Pipeline}
\end{figure}

在一个MapReduce工作开始时，其流水线的第一个任务会从分布式存储系统，比如HDFS中读取该工作的一部分数据。
之后在用户指定的mapper下对这部分数据进行操作，这部分成为map阶段。
在map阶段任务执行结束后，产生的中间结果会被保存到本地磁盘。
当所有map阶段的任务都执行完成，Hadoop MapReduce便会启动用户指定的reducer，开始reduce阶段的计算（此处不考虑slow-start）。
在reduce阶段的每个任务启动时，便会通过shuffle过程从远程节点的磁盘读取map阶段计算产生的中间结果。
当reduce阶段执行完成之后，产生的最终结果会被保存到分布式存储系统，为工作流水线的下一个步骤提供输入。
可以看到在经典的MapReduce计算过程中，每个reduce阶段结束之后多需要写入分布式存储系统，而且对于流水线中开始的map任务也存在大量的冗余操作。

Apache Tez\cite{tez}是基于Hadoop的生态系统，在Yarn的基础上\cite{yarn}的分布式DAG计算框架。
不同于MapReduce，Tez将原先的map和reduce操作拆分成了更细粒度的操作，并且通过顶点（vertex）和边的方式来表示运算的DAG。
具体执行DAG可以参考\ref{fig:tezdag}。
其执行DAG中的定点代表了用户定义的运算逻辑，每个顶点中包含了具体的并行执行的运算任务，来实现分布式计算加速。
而边则表示顶点之间的数据依赖，从数据生产者指向消费者，其中三种比较常见的依赖为“一对一”，“广播”和“分散-聚合”（one-to-one, broadcast, scatter-gather）。
图\ref{fig:tezdag}中Vertex 1和Vertex 3之间的依赖便是“一对一”依赖，而剩下的则是“分散-聚合”依赖，也就是shuffle依赖。
在Tez执行DAG的过程中，每个reduce任务将不在进行对分布式存储系统的写入操作，同时也通过复用map任务的部分重复性操作，显著提高了运行效率。

\begin{figure}[!htp]
    \centering
    \includegraphics[width=0.8\textwidth]{tezdag.pdf}
	\bicaption[fig:tezdag]{Apache Tez执行示意图}{Apache Tez执行示意图}{Fig}{Execution DAG of Apache Tez}
\end{figure}

Spark\cite{spark}的出现和开源（Apache Spark\cite{apachespark}）使分布式并行计算的效率和使用范围得到了进一步的提升。
在设计过程中，Spark为用户提供了丰富的接口（groupBy，sortByKey等），从而实现了更丰富的计算逻辑表达。
用户可以通过这些接口来对分布式工作进行编程设计。
在Spark的核心，用户的计算逻辑通过接口被翻译成了Resilient Distributed Datasets（RDD）之间的转换关系（transformation）。
连接RDD之间的转换关系最终在执行过程中行程系带（lineage），用来表示各个执行过程中的依赖。
这些依赖关系最终被调度器优化成DAG，用来表达逻辑层的计算过程。

图\ref{fig:sparkdag}展示了Spark在具体数据层执行过程。
Spark中的RDD在具体数据层中与计算所需的分布式数据集一一对应。
在工作执行过程中，Spark采用了延迟绑定的策略。
其调度器会从用户提交的最后一个RDD开始延系带关系递归向前，当遇到转换关系中存在全部依赖时（wide dependency）便将其作为计算阶段（stage）的分界。
该递归过程会在寻找到已经计算完成的RDD或者该计算工作的最初始输入时停止。
对于一个节点上同一个阶段内的所有RDD之间的转换计算，Spark将其统一成一个任务（task），由其负责对该阶段内一个数据分区的所有运算。
同时，Spark会在执行阶段将RDD所对应的数据块缓存在节点内存中，从而使一个任务的所有转换计算都能在内存中高效得进行。
当工作执行到存在全部依赖的时候，Spark会通过一次shuffle操作来满足下一个计算阶段的数据依赖。

\begin{figure}[!htp]
    \centering
    \includegraphics[width=0.8\textwidth]{sparkdag.pdf}
	\bicaption[fig:sparkdag]{Spark执行示意图}{Spark执行示意图}{Fig}{Execution DAG of Spark}
\end{figure}

在学术领域比较有代表性的一个分布式计算框架模型就是Dryad\cite{dryad}。
Dryad通过Nebula的脚本语言向用户提供接口。
用户可以通过该脚本语言来构造顶点和变，最终行程一个DAG。
其中顶点表示具体的某一种计算操作，而每一条边则代表顶点之间的数据依赖模式，具体可以通过本地临时文件，TCP管道或者共享内存来实现。
临时文件使其实现数据流动的默认方式，在一个顶点的执行结束之后就会在本地生成文件。
TCP管道和共享内存都可以避免磁盘访问，但是TCP管道需要不同顶点同时运行，而共享内存甚至需要不同顶点在同一个进程里运行。

Dryad框架在获取用户提交的DAG之后，会将DAG划分成多个阶段（stage），每个阶段都包含了一个特定的计算顶点。
在Dryad执行具体工作时，会对相应的DAG进行运行时的优化。
比如对其中的多对一的规约节点和一对多的节点进行副本操作，从而增加计算过程的并发度。
经过优化之后的具体执行流程图如图\ref{fig:dryaddag}所示。
通过Dryad的运行时优化之后，就会在每个阶段行程多个具有相同执行逻辑的顶点，而在默认采用临时文件进行数据传输时，就会形成典型的shuffle传输。

\begin{figure}[!htp]
    \centering
    \includegraphics[width=0.8\textwidth]{dryaddag.pdf}
	\bicaption[fig:dryaddag]{Dryad执行示意图}{Dryad执行示意图}{Fig}{Execution DAG of Dryad}
\end{figure}

Pregel\cite{pregel}主要针对于大规模的图计算而设计的图计算引擎。
其计算逻辑图也由顶点和边构成。
但是值得注意的是Pregel的计算逻辑图是支持顶点间有环存在的。
Pregel中的每个顶点代表一个用户定义并且支持修改的状态。
而其中的有向边则连接了两个顶点，用来进行信息的传递。
其具体计算步骤则根据整体同步并行计算模型（Bulk Synchronize Parallel，BSP）来控制。
如图\ref{fig:pregel}所示，Pregel的执行过程由若干个$Supterstep$组成。
每个$Supterstep$会根据全局的同步信号来进行信息传递。
计算过程最终会在所有节点的算法都终止时结束。
在每个$Supterstep$内，执行图中的所有节点都会并发的进行本地的计算。
图\ref{fig:pregel}中的虚线则代表了节点之间的有向边，即数据依赖。
当完成一个$Supterstep$之后，每个节点都会根据定义的边向目的节点发送信息，之后进入下一步的计算。
而在$Supterstep$之间，也需要完成一个多对多的顶点信息传输，也可以用shuffle来表示。

\begin{figure}[!htp]
    \centering
    \includegraphics[width=0.8\textwidth]{pregel.pdf}
	\bicaption[fig:dryaddag]{Pregel执行示意图}{Pregel执行示意图}{Fig}{Execution Model of Pregel}
\end{figure}

综合对上述这些分布式计算框架的特点研究，我们发现其中都存在一种多对多的网络传输模式，即shuffle。
在执行DAG的过程中，对于每一个计算阶段（stage）采用了整体同步并行计算模型(BSP)来对大数据进行分布式的并行批处理。
在一个计算阶段，框架会将数据分块，并且对于每一块数据应用相同的用户定义计算来进行并行处理
在DAG的执行过程中，每一个任务都会有对上一个任务的数据依赖。这个数据依赖可以被分成两个类型：完全依赖和部分依赖。
完全依赖指的是当前计算阶段的一个任务所需要的输入数据完全依赖于上一个计算阶段的一个或几个任务的所有输出数据，
部分依赖，也就是本文中提到的shuffle，则表示当前计算阶段的一个任务所需要的输入数据依赖于上一个计算阶段的多个任务的部分数据。
通常而言，对于产生shuffle数据的一端，我们称之为映射阶段（Map Stage），而对于接收shuffle数据的一端，我们称之为规约阶段（Reduce Stage）。
对于完全依赖，目前已经有了好的解决方案。比如Spark\cite{apachespark}中RDD\cite{spark}的解决方案，就通过将连续的完全依赖的计算阶段合并成一个大的计算阶段，并且通过内存计算的加速来优化了这部分数据依赖。

但是对于shuffle的依赖，虽然这些DAG计算框架在设计上存在很多不同，但是他们都是通过多对多的网络数据传输方式来实现。
而shuffle阶段由于受限于I/O设备性能的限制（磁盘，网络等），会对整个端到端的应用执行性能带来很大的额外开销。
虽然近几年针对分布式DAG计算框架的计算阶段学术界和企业界都提出了很多优化方案\cite{pacman, babu, quincy, sync}，但是对于shuffle阶段在实际应用中的优化却一直很不理想。
比如在Facebook公开的一个MapReduce运行数据分析中，shuffle平均占到了所有任务完成时间的33\%。
对于需要大量shuffle的一些任务，shuffle的开销最多可以占到整个任务完成时间的70\%\cite{managing}。

\section{研究内容}

本文通过对这些计算框架的研究，发现制约shuffle性能的主要是由于缺乏对于不同类型的硬件资源的细粒度的管理和调度。
在目前的DAG计算框架调度算法中，对于一个计算阶段的每一个任务，DAG计算框架的调度器都会分配集群中一部分固定的硬件资源，包括CPU，内存，磁盘和网络等。
为了简化调度算法，这些资源被捆绑成一个slot来进行粗粒度的管理和分配。
这种粗粒度的调度算法虽然简化了调度过程，但是也引入无法充分利用硬件资源的问题。
比如，当一个任务进行CPU内存密集的数据计算时，该slot所占用的I/O资源就会被限制。反之，当任务进行I/O密集的shuffle过程时，CPU和内存等计算资源就会被闲置。

除此之外，在shuffle阶段不可避免得会引入多对多的网络数据传输。
在目前的分布式DAG计算框架中，此阶段的网络数据传输也没有得到很好的管理。
当reduce阶段被调度并启动之后，集群中会有多个reduce任务同时启动，并且几乎同时通过网络来从远程节点获取数据。
这种几乎同步的数据传输模式会给集群的网络带来一个瞬时的流量高峰。
而当带宽有限的情况下，这种瞬时的高峰极易造成网络的拥塞，从而进一步减慢了数据传输的速率。

更糟糕的是，上述发现的问题存在于大部分主流的分布式DAG计算框架当中。所以仅仅只是针对其中某一个框架提出解决方案并不能很好的缓解shuffle给分布式计算带来的性能开销。

为了给shuffle过程提供一个具有普适性的优化方案，本文提出了S(huffle)Cache --- 一个开源的即插型shuffle管理系统来给不同的DAG计算框架提供高效的shuffle管理和优化。
具体来说，SCache通过提供跨框架的API设计，来接管在DAG计算过程中的shuffle阶段。
同时SCache采用了以下几点关键创新，来实现对于shuffle的高效管理和优化：

\begin{enumerate}
	\item 将shuffle从计算过程中的解耦。使得shuffle的过程独立到DAG计算框架外部进行管理，从而实现了更细粒度的硬件资源管理，提高硬件资源的复用率和利用率，进而加速shuffle过程。
	\item 结合应用的上下文对reduce任务进行预调度。采取了启发式算法，根据map阶段执行过程中的中间状态，结合应用的上下文逻辑和数据本地性等特征提前调度reduce阶段的任务。
	\item 对shuffle数据进行预取。在map执行阶段，根据启发式预调度算法的结果，对shuffle数据进行预取，既避免了同步数据读取给网络带来的压力，又能将大部分网络传输时间隐藏到计算的阶段。
	\item 采用了结合上下文的内存管理机制。根据DAG计算框架的任务调度策略，对不同的shuffle数据块设置优先级，同时提前将shuffle数据缓存在内存当中，加速shuffle数据的读取过程，提升任务计算性能。
	\item 设计了具有普适性的API。SCache不仅根据shuffle的读写为DAG计算框架设计了相应的API，同时也为DAG框架的调度器设计了相应提交shuffle相关元数据和获取预调度结果的API。
\end{enumerate}

\section{国内外研究现状}
\label{sec:relatedwork}

目前国内外的对shuffle的优化工作主要分成三个方向：提前调度，延迟调度和纯网络层面的优化。

\textbf{提前调度}：Slow-start作为Hadoop MapReduce\cite{hadoop}中最经典的shuffle优化是提前调度的代表性方案。
Starfish\cite{starfish}通过对数据的采样来自动调整MapReduce中的系统参数，比如slow-start的比例，map和reduce任务的比例等待。
DynMR\cite{dynmr}通过动态的在map阶段末端启动reduce任务来减少对与shuffle数据的等待。
以上所有这些方案都没有将I/O操作从计算密集型的任务中解耦，因而仍然在slot中留下显示的I/O等待时间。
而且由于计算资源有限，提前启动reduce任务会占用有限的slot，减慢map阶段的执行。
所以在设定何时启动提前调度的参数时，会受到应用，输入数据，当前硬件资源等条件的影响，不仅参数设置困难，优化效果也会有较大波动。
iShuffle\cite{ishuffle}采用了讲shuffle从reduce阶段解耦的方式，并且提供了一个中心控制器来调度shuffle。但是这个方案并不能和好的处理对多个shuffle的依赖。
iHadoop\cite{ihadoop}采用了激进地提前调度多个接下来的计算阶段的任务，从而是的shuffle数据的预取成为可能。
Drizzle\cite{drizzle}也采用了提前调度任务的方式来实现shuffle数据的预取。
但是在我们研究过程中发现，随机激进地调度任务可能会破坏DAG计算框架的负载均衡，从而减慢应用的性能。

\textbf{延迟调度}：Delay Scheduling\cite{delay}采用延迟分配任务来获取更好的数据本地性，进而减少了shuffle阶段的网络数据传输。
ShuffleWatcher\cite{shufflewatcher}会在网络将近饱和的时候延迟shuffle数据的获取。同时它能在调度任务时获取更好的本地行。
Quincy\cite{quincy}和Fair Scheduling\cite{preemptive}都可以通过优化map任务的调度来获取更好的shuffle数据本地性。
但是以上这些工作都不能消除在计算任务中显示的I/O操作。更重要的是，他们的优化效果会因为网络的性能好坏和计算数据的不同分布而产生波动。

\textbf{网络层优化}：Varys\cite{varys}，Aalo\cite{aalo}，CODA\cite{coda}，Seagull\cite{seagull}都能结合应用层信息提供shuffle在网络传输时的优化，从而加快shuffle的传输过程。
而DCTCP\cite{dctcp}，pFabric\cite{pfabric}，Pias\cite{pias}，CONGA\cite{conga}等工作能结合数据中心网络的特点在纯网络层降低网络流的平均完成时间，增加网络吞吐。
虽然这些优化仅仅局限在shuffle过程中的网络传输部分，但是它们可以为本研究的优化提供进一步的性能提升。

\section{文章结构}

本文余下内容结构如下：

第二章介绍shuffle在分布式并行DAG计算框架中的特点以及相关背景。通过对shuffle特性的分析，来挖掘其可优化的空间和制定具体优化方案。

第三章详细介绍了SCache的具体设计与实现，包括SCache的架构，shuffle调度算法设计，接口设计以及内存管理策略。
同时还会介绍SCache和Spark协同工作的实例。

第四章对SCache在Spark平台上的优化效果进行了实验和分析。
通过仿真实验和Amazon AWS EC2上虚拟机集群的测试，验证了SCache在shuffle上的优化效果。

第五章对全文进行了总结，并且对未来的工作方向做出展望。







